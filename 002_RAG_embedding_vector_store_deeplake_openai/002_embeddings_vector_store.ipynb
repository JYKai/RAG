{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"llm.txt\"\n",
    "\n",
    "with open(source_text, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "chunked_text = [text[i: i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_path = \"hub://secufibre/space_exploration_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store exists\n"
     ]
    }
   ],
   "source": [
    "from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
    "import deeplake.util\n",
    "\n",
    "try:\n",
    "    vector_store = VectorStore(path=vector_store_path)\n",
    "    print(\"Vector store exists\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Vector store does not exist. You can create it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def embedding_function(texts, model=\"text-embedding-3-small\"):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "    return [data.embedding for data in openai.embeddings.create(input=texts, model=model).data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1654 embeddings in 4 batches of size 500:: 100%|██████████| 4/4 [01:22<00:00, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://secufibre/space_exploration_v1', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      "   text       text      (1654, 1)      str     None   \n",
      " metadata     json      (1654, 1)      str     None   \n",
      " embedding  embedding  (1654, 1536)  float32   None   \n",
      "    id        text      (1654, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_to_vector_store = True\n",
    "if add_to_vector_store == True:\n",
    "    with open(source_text, 'r') as f:\n",
    "        text = f.read()\n",
    "        CHUNK_SIZE = 1000\n",
    "        chunked_text = [text[i: i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]\n",
    "\n",
    "vector_store.add(\n",
    "    text=chunked_text,\n",
    "    embedding_function=embedding_function,\n",
    "    embedding_data=chunked_text,\n",
    "    metadata=[{\"source\": source_text}]*len(chunked_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://secufibre/space_exploration_v1', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      "   text       text      (1654, 1)      str     None   \n",
      " metadata     json      (1654, 1)      str     None   \n",
      " embedding  embedding  (1654, 1536)  float32   None   \n",
      "    id        text      (1654, 1)      str     None   \n"
     ]
    }
   ],
   "source": [
    "vector_store.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/secufibre/space_exploration_v1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://secufibre/space_exploration_v1 loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "ds = deeplake.load(vector_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = ds.size_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size in megabytes: 55.31311 MB\n",
      "Dataset size in gigabytes: 0.05402 GB\n"
     ]
    }
   ],
   "source": [
    "ds_size_mb = ds_size / 1024 ** 2\n",
    "print(f\"Dataset size in megabytes: {ds_size_mb:.5f} MB\")\n",
    "\n",
    "ds_size_gb = ds_size / 1024 ** 3\n",
    "print(f\"Dataset size in gigabytes: {ds_size_gb:.5f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
