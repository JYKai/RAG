{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100   875  100   875    0     0   2632      0 --:--:-- --:--:-- --:--:--  2627\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py\"\n",
    "output_file = \"grequests.py\"\n",
    "\n",
    "curl_command = [\n",
    "    \"curl\",\n",
    "    \"-o\",\n",
    "    output_file,\n",
    "    url\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.run(curl_command, check=True)\n",
    "    print(\"Download successful.\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Failed to download the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URLs of the Wikipedia articles mapped to keywords\n",
    "urls = {\n",
    "    \"prompt engineering\": \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n",
    "    \"artificial intelligence\":\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"llm\": \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"llms\": \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
    "}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_clean(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "\n",
    "    for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n",
    "        section = content.find('span', {'id': section_title})\n",
    "        if section:\n",
    "            for sib in section.parent.find_next_siblings():\n",
    "                sib.decompose()\n",
    "            section.parent.decompose()\n",
    "    \n",
    "    paragraphs = content.find_all('p')\n",
    "    cleaned_text = ' '.join(paragraph.get_text(seperator=' ', strip=True) for paragraph in paragraphs)\n",
    "    cleaned_text = re.sub(r'\\[\\d+\\]', '', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def process_query(user_input, num_words):\n",
    "    user_input = user_input.lower()\n",
    "    matched_keyword = next((keyword for keyword in urls if keyword in user_input), None)\n",
    "    if matched_keyword:\n",
    "        print(f\"Fetching data from: {urls[matched_keyword]}\")\n",
    "        cleaned_text = fetch_and_clean(urls[matched_keyword])\n",
    "        words = cleaned_text.split()\n",
    "        first_n_words = ' '.join(words[:num_words])\n",
    "        wrapped_text = textwrap.fill(first_n_words, width=80)\n",
    "        print(\"\\nFirst {} words of the cleaned text:\".format(num_words))\n",
    "        print(wrapped_text)\n",
    "\n",
    "        prompt = f\"Summarize the following information about {matched_keyword}:\\n{first_n_words}\"\n",
    "        wrapped_prompt = textwrap.fill(prompt, width=80)\n",
    "        print(\"\\nPrompt for Generator:\", wrapped_prompt)\n",
    "        return first_n_words\n",
    "    else:\n",
    "        print(\"No relevant keywords found. Please enter a query related to 'LLM', 'LLMs', or 'Prompt Engineering'\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Enter your query: \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ranking >= 1 and ranking < 3:\n",
    "    text_input = user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 'human_feedback.txt' successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100  1496  100  1496    0     0   4205      0 --:--:-- --:--:-- --:--:--  4202\n"
     ]
    }
   ],
   "source": [
    "hf = False\n",
    "if ranking >= 3 and ranking < 5:\n",
    "    hf = True\n",
    "\n",
    "if hf:\n",
    "    from grequests import download\n",
    "    directory = \"Chapter05\"\n",
    "    filename = \"human_feedback.txt\"\n",
    "    download(directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model (LLM) is an advanced AI system trained on vast amounts of text data to generate human-like text responses. It understands and generates language based on the patterns and information it has learned during training. LLMs are highly effective in various language-based tasks, including answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. It can be programmed to handle common technical questions about the C-phone series, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can be used to gather customer feedback, providing valuable insights into user experiences and product performance. This feedback can then be used to improve products and services. Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if hf:\n",
    "    efile = os.path.exists(\"human_feedback.txt\")\n",
    "\n",
    "    if efile:\n",
    "        with open(\"human_feedback.txt\", \"r\") as file:\n",
    "            content = file.read().replace(\"\\n\", \" \").replace(\"#\", \"\")\n",
    "        text_input = content\n",
    "        print(text_input)\n",
    "    else:\n",
    "        print(\"File not found\")\n",
    "        hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ranking>=5:\n",
    "  max_words=100 #Limit: the size of the data we can add to the input\n",
    "  rdata=process_query(user_input, max_words)\n",
    "  print(rdata) # for maintenance if necessary\n",
    "  if rdata:\n",
    "        rdata_clean = rdata.replace('\\n', ' ').replace('#', '')\n",
    "        rdata_sentences = rdata_clean.split('. ')\n",
    "  text_input=rdata\n",
    "  print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input: what is an llm?\n"
     ]
    }
   ],
   "source": [
    "print(\"user input:\",user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text input: A Large Language Model (LLM) is an advanced AI system trained on vast amounts of text data to generate human-like text responses. It understands and generates language based on the patterns and information it has learned during training. LLMs are highly effective in various language-based tasks, including answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. It can be programmed to handle common technical questions about the C-phone series, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can be used to gather customer feedback, providing valuable insights into user experiences and product performance. This feedback can then be used to improve products and services. Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty. \n"
     ]
    }
   ],
   "source": [
    "print(\"text input:\",text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Time: 6.16 seconds\n",
      "gpt-4o Response:  A Large Language Model (LLM) is an advanced artificial intelligence system designed to generate human-like text responses by being trained on extensive text data. These models understand and produce language based on patterns and information learned during their training. LLMs are highly effective in various language-based tasks such as answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.\n",
      "\n",
      "In the context of customer support for the C-phone series, integrating an LLM can significantly enhance service quality and efficiency. A conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and allowing human agents to focus on more complex issues. It can be programmed to handle common technical questions, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can gather customer feedback, providing valuable insights into user experiences and product performance, which can be used to improve products and services.\n",
      "\n",
      "Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "gptmodel = \"gpt-4o\"\n",
    "start_time = time.time()\n",
    "\n",
    "def call_gpt4_with_full_text(itext):\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please summarize or elaborate on the following content:\\n{text_input}\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gptmodel,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "    \n",
    "gpt4_response = call_gpt4_with_full_text(text_input)\n",
    "\n",
    "response_time = time.time() - start_time\n",
    "print(f\"Response Time: {response_time:.2f} seconds\")\n",
    "print(gptmodel, \"Response: \", gpt4_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Response:\n",
      "---------------\n",
      "A Large Language Model (LLM) is an advanced artificial intelligence system\n",
      "designed to generate human-like text responses by being trained on extensive\n",
      "text data. These models understand and produce language based on patterns and\n",
      "information learned during their training. LLMs are highly effective in various\n",
      "language-based tasks such as answering questions, making recommendations, and\n",
      "facilitating conversations. They can be continually updated with new information\n",
      "and trained to understand specific domains or industries.  In the context of\n",
      "customer support for the C-phone series, integrating an LLM can significantly\n",
      "enhance service quality and efficiency. A conversational agent powered by an LLM\n",
      "can provide instant responses to customer inquiries, reducing wait times and\n",
      "allowing human agents to focus on more complex issues. It can be programmed to\n",
      "handle common technical questions, troubleshoot problems, guide users through\n",
      "setup processes, and offer tips for optimizing device performance. Additionally,\n",
      "it can gather customer feedback, providing valuable insights into user\n",
      "experiences and product performance, which can be used to improve products and\n",
      "services.  Furthermore, the LLM can be designed to escalate issues to human\n",
      "agents when necessary, ensuring that customers receive the best possible support\n",
      "at all levels. The agent can also provide personalized recommendations for\n",
      "customers based on their usage patterns and preferences, enhancing user\n",
      "satisfaction and loyalty.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"GPT-4 Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")\n",
    "\n",
    "# Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
    "print_formatted_response(gpt4_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Time: 6.16 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response Time: {response_time:.2f} seconds\")  # Print response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Example usage with your existing functions\n",
    "similarity_score = calculate_cosine_similarity(text_input, gpt4_response)\n",
    "\n",
    "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 20\n",
    "score_history = 60\n",
    "score_threshold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      "A Large Language Model (LLM) is an advanced artificial intelligence system designed to generate human-like text responses by being trained on extensive text data. These models understand and produce language based on patterns and information learned during their training. LLMs are highly effective in various language-based tasks such as answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.\n",
      "\n",
      "In the context of customer support for the C-phone series, integrating an LLM can significantly enhance service quality and efficiency. A conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and allowing human agents to focus on more complex issues. It can be programmed to handle common technical questions, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can gather customer feedback, providing valuable insights into user experiences and product performance, which can be used to improve products and services.\n",
      "\n",
      "Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty.\n",
      "\n",
      "Please evaluate the response based on the following criteria:\n",
      "1 - Poor, 2 - Fair, 3 - Good, 4 - Excellent, 5 - Perfect\n",
      "Evaluator Score:  3\n",
      "Rankings            : 22\n",
      "Score history       : 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_response(response):\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(response)\n",
    "    print(\"\\nPlease evaluate the response based on the following criteria:\")\n",
    "    print(\"1 - Poor, 2 - Fair, 3 - Good, 4 - Excellent, 5 - Perfect\")\n",
    "    score = input(\"Enter the relevance and coherence score (1-5): \")\n",
    "    try:\n",
    "        score = int(score)\n",
    "        if 1 <= score <= 5:\n",
    "            return score\n",
    "        else:\n",
    "            print(\"Invalid score. Please enter a number between 1 and 5.\")\n",
    "            return evaluate_response(response)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid number.\")\n",
    "        return evaluate_response(response)\n",
    "\n",
    "score = evaluate_response(gpt4_response)\n",
    "print(\"Evaluator Score: \", score)\n",
    "\n",
    "counter += 1\n",
    "score_history += score\n",
    "mean_score = round(np.mean(score_history/counter), 2)\n",
    "if counter > 0:\n",
    "    print(\"Rankings            :\", counter)\n",
    "    print(\"Score history       :\", mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   809  100   809    0     0   2335      0 --:--:-- --:--:-- --:--:--  2338\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 'thumbs_up.png' successfully.\n",
      "Downloaded 'thumbs_down.png' successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100   810  100   810    0     0   2364      0 --:--:-- --:--:-- --:--:--  2361\n"
     ]
    }
   ],
   "source": [
    "from grequests import download\n",
    "\n",
    "# Define your variables\n",
    "directory = \"commons\"\n",
    "filename = \"thumbs_up.png\"\n",
    "download(directory, filename)\n",
    "\n",
    "# Define your variables\n",
    "directory = \"commons\"\n",
    "filename = \"thumbs_down.png\"\n",
    "download(directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_threshold = 10\n",
    "score_threshold = 4\n",
    "\n",
    "if counter > counter_threshold and score_history <= score_threshold:\n",
    "    print(\"Human expert evaluation is required for the feedback loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
